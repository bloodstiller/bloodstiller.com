<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Homelabs on Hack Me Daddy</title>
    <link>http://localhost:1313/homelab/</link>
    <description>Recent content in Homelabs on Hack Me Daddy</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/homelab/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to Run Local LLMs with Ollama on AMD GPU (Complete Guide)</title>
      <link>http://localhost:1313/homelab/runningollamalocally/</link>
      <pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/homelab/runningollamalocally/</guid>
      <description>&lt;p&gt;This is mainly being put here for reference for me. I will writeup nix instructions when I finally migrate my main system to nix, but at the moment this is on arch &amp;amp; I wanted to document the process for myself.&lt;/p&gt;&#xA;&lt;p&gt;I am using an AMD gpu so this may differ for you:&lt;/p&gt;&#xA;&lt;h2 id=&#34;install-amd-gpu-backend-packages&#34;&gt;Install AMD GPU backend packages:&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo pacman -S rocminfo rocm-opencl-sdk rocm-hip-sdk rocm-ml-sdk&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;install-ollama-with-amd-gpu-support-dot&#34;&gt;Install ollama with AMD GPU support.&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yay -S ollama-rocm&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;start-ollama&#34;&gt;Start Ollama:&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama serve&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;run-open-webui-via-docker&#34;&gt;Run open-webui via docker:&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;services&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;open-webui&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;build&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;context&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;dockerfile&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Dockerfile&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;container_name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;open-webui&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;./open-webui:/app/backend/data&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;network_mode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;host&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;environment&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;OLLAMA_BASE_URL=http://127.0.0.1:11434&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;restart&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;unless-stopped&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;open-webui&lt;/span&gt;: {}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;Docker compose up -d&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;access-open-webui&#34;&gt;Access Open WebUI:&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;http://localhost:8080&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;pull-a-model-down&#34;&gt;Pull A Model Down:&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/ox-hugo/2025-03-10-080912_.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;system-requirements&#34;&gt;System Requirements&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Minimum 8GB RAM (16GB+ recommended for larger models)&lt;/li&gt;&#xA;&lt;li&gt;GPU with at least 6GB VRAM for running medium-sized models&lt;/li&gt;&#xA;&lt;li&gt;Storage space depending on models (each model can be 3-8GB+)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;common-model-commands&#34;&gt;Common Model Commands&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# List all installed models&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama list&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Remove a model&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama rm model-name&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Get model information&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama show model-name&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run a model in CLI&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama run model-name&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If GPU is not detected, ensure ROCm drivers are properly installed and configured&lt;/li&gt;&#xA;&lt;li&gt;Check logs with &lt;code&gt;journalctl -u ollama&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify Ollama service status with &lt;code&gt;systemctl status ollama&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Common ports used: 11434 (Ollama API), 8080 (Open WebUI)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Official Ollama documentation: &lt;a href=&#34;https://github.com/ollama/ollama/tree/main/docs&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;https://github.com/ollama/ollama/tree/main/docs&lt;/a&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Model library:  &lt;a href=&#34;https://ollama.com/search&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;https://ollama.com/search&lt;/a&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;GitHub repository: &lt;a href=&#34;https://github.com/ollama/ollama&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;https://github.com/ollama/ollama&lt;/a&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;This guide demonstrates how to run Large Language Models (LLMs) locally using Ollama with AMD GPU acceleration. While many guides focus on NVIDIA GPUs, this tutorial specifically covers AMD GPU setup using ROCm on Arch Linux. Running LLMs locally provides better privacy, reduced latency, and no API costs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rebuilding My Homelab In NixOS (Part 4) Decrypting boot early with initrd-ssh</title>
      <link>http://localhost:1313/homelab/20250280118-nixos-homelab-4/</link>
      <pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/homelab/20250280118-nixos-homelab-4/</guid>
      <description>&lt;p&gt;This is part 4 of my series demonstrating my rebuild of my homelab in NixOS.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If you would like to see the initial creation of the VM on Proxmox please see &lt;a href=&#34;https://bloodstiller.com/homelab/20250220118-nixos-homelab/&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;Part 1&lt;/a&gt;&#xA;.&lt;/li&gt;&#xA;&lt;li&gt;Enabling SSH access on NixOS &lt;a href=&#34;https://bloodstiller.com/homelab/20250220118-nixos-homelab-2/&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;Part 2&lt;/a&gt;&#xA;.&lt;/li&gt;&#xA;&lt;li&gt;Mounting a secondary drive on NixOS &lt;a href=&#34;https://bloodstiller.com/homelab/20250280118-nixos-homelab-3/&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;Part 3&lt;/a&gt;&#xA;.&lt;/li&gt;&#xA;&lt;li&gt;Decrypting boot early with initrd-ssh (this post)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;remote-luks-unlocking-for-nixos-homelab-systems&#34;&gt;Remote LUKS Unlocking for NixOS Homelab Systems&lt;/h2&gt;&#xA;&lt;h3 id=&#34;the-challenge-with-encrypted-homelab-systems&#34;&gt;The Challenge with Encrypted Homelab Systems:&lt;/h3&gt;&#xA;&lt;p&gt;Running an encrypted NixOS system (or any encrypted Linux) in a homelab environment presents a specific challenge: entering the LUKS passphrase at boot time typically requires either physical access to the machine or access to a management console like Proxmox.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rebuilding My Homelab In NixOS (Part 3) Mounting A Secondary Drive At Boot</title>
      <link>http://localhost:1313/homelab/20250280118-nixos-homelab-3/</link>
      <pubDate>Fri, 28 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/homelab/20250280118-nixos-homelab-3/</guid>
      <description>&lt;p&gt;This is part 3 of my series demonstrating my rebuild of my homelab in NixOS.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If you would like to see the initial creation of the VM on Proxmox please see &lt;a href=&#34;https://bloodstiller.com/homelab/20250220118-nixos-homelab/&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;Part 1&lt;/a&gt;&#xA;.&lt;/li&gt;&#xA;&lt;li&gt;Enabling SSH access on NixOS &lt;a href=&#34;https://bloodstiller.com/homelab/20250220118-nixos-homelab-2/&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;Part 2&lt;/a&gt;&#xA;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;mounting-a-second-drive-in-nixos&#34;&gt;Mounting A Second Drive In NixOS:&lt;/h2&gt;&#xA;&lt;p&gt;So far we have built our VM, installed NixOS and configured it for SSH authentication. We still have a little more basic configuration to do before we can progress further. We need to mount the secondary drive, luckily this is a quick process and NixOS has a handy tool that helps us make this mount permanent and it doesn&amp;rsquo;t involve manually configuring fstab for change!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rebuilding My Homelab In NixOS (Part 2) Enabling SSH Login in NixOS</title>
      <link>http://localhost:1313/homelab/20250220118-nixos-homelab-2/</link>
      <pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/homelab/20250220118-nixos-homelab-2/</guid>
      <description>&lt;p&gt;This is part 2 of my series demonstrating my rebuild of my homelab in NixOS. If you would like to see the initial creation of the VM on Proxmox please see &lt;a href=&#34;https://bloodstiller.com/homelab/20250220118-nixos-homelab/&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;Part 1&lt;/a&gt;&#xA;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;first-login-to-nixos&#34;&gt;First Login to NixOS:&lt;/h2&gt;&#xA;&lt;p&gt;Lets start off where we left off in the last section, and we will login to the NixOS host for the first time.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/ox-hugo/2025-02-26-084309_.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;As we are using Proxmox our first login will be via using the console, use the credentials you provided in the VM creation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Autostart Tailscale on NixOS system boot &amp; rebuild</title>
      <link>http://localhost:1313/homelab/202050222_tailscalenixos/</link>
      <pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/homelab/202050222_tailscalenixos/</guid>
      <description>&lt;p&gt;Are you tired of running &lt;code&gt;sudo tailscale up&lt;/code&gt; every time you login, I know I am. So I thought why spend under two seconds waiting for something to run and using auto complete with ZSH to easily find the command when I can easily create a service that launches Tailscale on boot for me and re-launches on every rebuild of the system.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;This configuration will ensure that&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Tailscale starts automatically with your system.&lt;/li&gt;&#xA;&lt;li&gt;You don&amp;rsquo;t need to manually authenticate each time.&lt;/li&gt;&#xA;&lt;li&gt;Your authentication key is stored securely using &lt;a href=&#34;https://getsops.io/&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;sops&lt;/a&gt;&#xA;/&lt;a href=&#34;https://github.com/Mic92/sops-nix?tab=readme-ov-file&#34;  target=&#34;_blank&#34; rel=&#34;noreferrer nofollow&#34;&gt;sops-nix&lt;/a&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;The connection is established only when needed (won&amp;rsquo;t try to reconnect if already connected).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;This assumes you already have a Tailscale account and sops setup, if you don&amp;rsquo;t please see guides below.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rebuilding My Homelab In NixOS (Part 1)</title>
      <link>http://localhost:1313/homelab/20250220118-nixos-homelab/</link>
      <pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/homelab/20250220118-nixos-homelab/</guid>
      <description>&lt;h2 id=&#34;the-catalyst&#34;&gt;The Catalyst:&lt;/h2&gt;&#xA;&lt;p&gt;I have stepped away from doing writeups for a little while, purely due to wanting to mix things up and the fact that I was just grinding for a year prior to get my CPTS certification; I actually have a number of writeups for active boxes written just not on the site due to the terms and conditions by HTB. And as I needed something else to spend my time with I created a Proxmox cluster using two nodes (I know I know you need 3 nodes for it actually be a cluster) &amp;amp; also set up a Proxmox backup server. However that took all of an hour to do, honestly, it was one of the easiest setups I have ever done (I will create a writeup for this soon enough.)&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
